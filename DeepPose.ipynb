{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepPose.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1WrAe8lI3tb"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import scipy.io\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VYo4wS1MPau"
      },
      "source": [
        "!wget 'https://sam.johnson.io/research/lsp_dataset.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGN32NsYMfr9"
      },
      "source": [
        "!unzip lsp_dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQLgv3gXZ8MM"
      },
      "source": [
        "\n",
        "def get_data(start=0,noi=10):\n",
        "\n",
        "  master_dir = '/content/images/'\n",
        "\n",
        "  imgs_files = os.listdir(master_dir)[start:noi]\n",
        "  all_imgs = []\n",
        "  coords = []\n",
        "\n",
        "  mat = scipy.io.loadmat('/content/joints.mat')\n",
        "  joint_data = np.array(mat['joints'])\n",
        "\n",
        "  x=[]\n",
        "  y=[]\n",
        "\n",
        "  for each_file in imgs_files:\n",
        "\n",
        "    res = re.findall( r'([a-z]*)([0-9]*).jpg',each_file )\n",
        "    idx = int(res[0][-1])\n",
        "    index = idx-1\n",
        "\n",
        "    img = Image.open( master_dir+each_file )\n",
        "    w,h = img.size\n",
        "\n",
        "    img_arr = np.array( img.resize((220,220)) )/255.0\n",
        "    all_imgs.append( img_arr )\n",
        "\n",
        "    \n",
        "    coords.append( np.concatenate( (joint_data[0,:,index]/w , joint_data[1,:,index]/h),axis=0 ) )\n",
        "\n",
        "  return all_imgs,coords\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJwHkbgmbfTc"
      },
      "source": [
        "imgs,coords = get_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LMBhWZsv5vj"
      },
      "source": [
        "print( len(coords) )\n",
        "print( coords[0] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neHnSwJbbhm8"
      },
      "source": [
        "# View some sample images\n",
        "for j in range(10):\n",
        "\n",
        "  show = plt.imshow(imgs[j])\n",
        "  for i in range( 14 ):\n",
        "    \n",
        "    plt.plot( int(coords[j][i]*220),int(coords[j][14+i]*220) ,marker='o' )\n",
        "  \n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF_Qwqofrgrd"
      },
      "source": [
        "def shuffle_data(x,y):\n",
        "\n",
        "  shuffle_x = []\n",
        "  shuffle_y = []\n",
        "\n",
        "  perm_index = np.random.permutation( len(y) )\n",
        "\n",
        "  for i in perm_index:\n",
        "    shuffle_x.append( x[i] )\n",
        "    shuffle_y.append( y[i] )\n",
        "\n",
        "  return shuffle_x,shuffle_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP4aU5ObI_sl"
      },
      "source": [
        "init = tf.keras.initializers.he_uniform(seed=5)\n",
        "inputs = tf.keras.layers.Input((220,220,3))\n",
        "\n",
        "out = tf.keras.layers.Conv2D(32,(5,5),strides=(1,1),activation=\"relu\",kernel_regularizer=tf.keras.regularizers.l2(0.001),kernel_initializer=init)(inputs)\n",
        "out = tf.keras.layers.BatchNormalization()(out)\n",
        "\n",
        "out = tf.keras.layers.Conv2D(64,(3,3),strides=(1,1),activation=\"relu\",kernel_regularizer=tf.keras.regularizers.l2(0.001),kernel_initializer=init)(out)\n",
        "out = tf.keras.layers.BatchNormalization()(out)\n",
        "out = tf.keras.layers.MaxPool2D((2,2))(out)\n",
        "\n",
        "out = tf.keras.layers.Conv2D(128,(3,3),strides=(1,1),activation=\"relu\",kernel_regularizer=tf.keras.regularizers.l2(0.001),kernel_initializer=init)(out)\n",
        "out = tf.keras.layers.BatchNormalization()(out)\n",
        "out = tf.keras.layers.MaxPool2D((2,2))(out)\n",
        "\n",
        "out = tf.keras.layers.Conv2D(128,(3,3),strides=(1,1),activation=\"relu\",kernel_regularizer=tf.keras.regularizers.l2(0.001),kernel_initializer=init)(out)\n",
        "out = tf.keras.layers.BatchNormalization()(out)\n",
        "out = tf.keras.layers.MaxPool2D((2,2))(out)\n",
        "\n",
        "out = tf.keras.layers.Conv2D( 256,(3,3),activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.001) ,kernel_initializer=init )(out)\n",
        "out = tf.keras.layers.BatchNormalization()(out)\n",
        "\n",
        "out = tf.keras.layers.Conv2D(256,(3,3),activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.001) ,kernel_initializer=init )(out)\n",
        "out = tf.keras.layers.MaxPool2D((2,2))(out)\n",
        "\n",
        "out = tf.keras.layers.Conv2D(512,(3,3),activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.001) ,kernel_initializer=init )(out)\n",
        "\n",
        "out = tf.keras.layers.Conv2D(512,(3,3),activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.001) ,kernel_initializer=init)(out)\n",
        "out = tf.keras.layers.MaxPool2D((2,2))(out)\n",
        "\n",
        "out = tf.keras.layers.Flatten()(out)\n",
        "\n",
        "out = tf.keras.layers.Dense(512,activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.001) ,kernel_initializer=init )(out)\n",
        "out = tf.keras.layers.Dense(256,activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.001) ,kernel_initializer=init )(out)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(28,kernel_initializer=init)(out)\n",
        "\n",
        "model = 0\n",
        "model = tf.keras.Model(inputs,outputs)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUIu3Saxln2c"
      },
      "source": [
        "model = tf.keras.models.load_model( '/content/deepPose (1).h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2GPkxbgEWJs"
      },
      "source": [
        "cost_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "# Custom defined cost \n",
        "# def cost_fn(y_true,y_pred):\n",
        "#   locost = (1/y_true.shape[0])*tf.reduce_sum( tf.square(y_true- tf.cast(y_pred,tf.float64) ) )\n",
        "#   return locost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR8CXnAg7sOQ"
      },
      "source": [
        "def val_test( model,test_x,test_y ):\n",
        "\n",
        "  mean = tf.keras.metrics.Mean()\n",
        "  mean.reset_states()\n",
        "  # Batch size\n",
        "  b_size = 8\n",
        "  test_nob = int( len(test_x)/b_size )\n",
        "  test_nob\n",
        "\n",
        "  for n in range(test_nob):\n",
        "\n",
        "    x_single = tf.convert_to_tensor( test_x[n*b_size:(n+1)*b_size] )\n",
        "    y_single = tf.convert_to_tensor( test_y[n*b_size:(n+1)*b_size] )\n",
        "\n",
        "    test_pred = model(x_single)\n",
        "    pred_hot = cost_fn(y_single*220,test_pred*220)\n",
        "\n",
        "    mean.update_state( pred_hot )    \n",
        "\n",
        "  return mean.result()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxoyt95i8h0z"
      },
      "source": [
        "test_img,test_coods = get_data(1800,1999)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4GOHBRmEom2"
      },
      "source": [
        "len(test_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ0lqsy4B0FH"
      },
      "source": [
        "test_loss = val_test(model,test_img,test_coods)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdPzC9HXCvK_"
      },
      "source": [
        "# Loss on untrained model\n",
        "print(test_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZOdCbzjKUyJ"
      },
      "source": [
        "epochs = 30\n",
        "l_rate = 0.0001\n",
        "# MAX 2000\n",
        "num_of_ex = 1800 \n",
        "batch_size = 8\n",
        "nob = int(num_of_ex/batch_size) \n",
        "\n",
        "optimizer = tf.keras.optimizers.RMSprop(l_rate)\n",
        "mean = tf.keras.metrics.Mean()\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "\n",
        "  data_img,coords = get_data(0,num_of_ex)\n",
        "\n",
        "  print( len(data_img) )\n",
        "  print( data_img[0].shape )\n",
        "  \n",
        "  print( len(coords) )\n",
        "  print( len(coords[0]) )\n",
        "\n",
        "\n",
        "  for e in range(epochs):\n",
        "\n",
        "    ctr = 0\n",
        "    mean.reset_states()\n",
        "\n",
        "    shuffle_x,shuffle_y = shuffle_data( data_img,coords )\n",
        "\n",
        "    for b in range( nob ):\n",
        "\n",
        "      x_batch = tf.convert_to_tensor( shuffle_x[ b*batch_size:(b+1)*batch_size ] )\n",
        "      y_batch = tf.convert_to_tensor( shuffle_y[ b*batch_size:(b+1)*batch_size ] )\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "\n",
        "        pred = model(x_batch)\n",
        "        \n",
        "        # estimation of size to be given\n",
        "        total_loss = cost_fn( y_batch*220 ,pred*220 )\n",
        "    \n",
        "      mean.update_state( total_loss )\n",
        "      grad = tape.gradient(total_loss,model.trainable_variables)\n",
        "      optimizer.apply_gradients( zip(grad,model.trainable_variables) )\n",
        "\n",
        "      if ctr%200==0:\n",
        "        \n",
        "        show = plt.imshow( x_batch[0].numpy() )\n",
        "        for i in range( 14 ):\n",
        "          \n",
        "          plt.plot( int( (pred[0][i])*220 ) , int( (coords[0][14+i])*220) ,marker='o' )\n",
        "\n",
        "        plt.show()\n",
        "        \n",
        "        show = plt.imshow( x_batch[0].numpy() )\n",
        "        for i in range( 14 ):\n",
        "          \n",
        "          plt.plot( int( (y_batch[0][i])*220 ) , int( (y_batch[0][14+i])*220) ,marker='o' )\n",
        "        plt.show()\n",
        "\n",
        "        print( \"loss is ==>  {}\".format(mean.result()) )\n",
        "\n",
        "      ctr = ctr+1\n",
        "\n",
        "    print(\"Train Loss is === >  {}\".format(mean.result()))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYip-Ry8WFQV"
      },
      "source": [
        "model.save('deepPose.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqjv_tPDGk00"
      },
      "source": [
        "model.trainable_variables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c4xUrmCawpn"
      },
      "source": [
        "test_loss = val_test(model,test_img,test_coods)\n",
        "print(\"Test  Loss is === >  {}\".format(test_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbNR0KGGyUTQ"
      },
      "source": [
        "def predict_one(img_arr):\n",
        "\n",
        "  tf_img = tf.convert_to_tensor(img_arr)\n",
        "  pred_one = model(img_arr)\n",
        "  print(pred_one.shape)\n",
        "\n",
        "  show = plt.imshow( img_arr[0] )\n",
        "  for i in range( 14 ):\n",
        "    \n",
        "    plt.plot( int( (pred_one[0][i])*220 ) , int( (pred_one[0][14+i])*220) ,marker='o' )\n",
        "\n",
        "  plt.show()\n",
        "  \n",
        "  show = plt.imshow( img_arr[0] )\n",
        "  plt.show()\n",
        "\n",
        "  return\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKj1GR3wnHMe"
      },
      "source": [
        "# test_img,test_coods = get_data(1800,1999)\n",
        "np_img = np.expand_dims( data_img[10] , 0 )\n",
        "print(np_img.shape)\n",
        "predict_one(np_img)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}