{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_GAN_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "avv5-YyMg5fp"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOPvvhVf9iv-"
      },
      "source": [
        "\"\"\"\n",
        "    Original GAN paper -- 'https://arxiv.org/abs/1406.2661'\n",
        "    But these GAN are hard to train.\n",
        "\n",
        "    So some tried and tested rule of thumbs shuld be followed to get results.\n",
        "    This Notebook follow these hacks -- 'https://github.com/soumith/ganhacks'\n",
        "\n",
        "    This Notebook is based on DCGANs paper -- 'https://arxiv.org/abs/1511.06434'\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gChfBguSkCDe"
      },
      "source": [
        "\"\"\"\n",
        "  FOR MNIST DATASET - DISCRIMINATOR MODEL\n",
        "\"\"\"\n",
        "\n",
        "inputs = tf.keras.layers.Input((28,28,1))\n",
        "\n",
        "out = tf.keras.layers.Conv2D( 64,(5,5),strides=(2,2), padding='same' )(inputs)\n",
        "out = tf.keras.layers.LeakyReLU()(out)\n",
        "out = tf.keras.layers.Dropout(0.3)(out)\n",
        "\n",
        "out = tf.keras.layers.Conv2D( 128,(5,5),strides=(2,2) , padding='same' )(out)\n",
        "out = tf.keras.layers.LeakyReLU()(out)\n",
        "out = tf.keras.layers.Dropout(0.3)(out)\n",
        "\n",
        "out = tf.keras.layers.Flatten()(out)\n",
        "outputs = tf.keras.layers.Dense( 1 )(out)\n",
        "\n",
        "D_model=0\n",
        "D_model = tf.keras.Model(inputs=inputs,outputs=outputs)\n",
        "D_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDVLBf9GlStw"
      },
      "source": [
        "\"\"\"\n",
        "  FOR MNIST DATASET - GENERATOR MODEL\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "inputs = tf.keras.layers.Input((100,))\n",
        "\n",
        "out = tf.keras.layers.Dense(7*7*256)(inputs)\n",
        "out = tf.keras.layers.BatchNormalization()(out)\n",
        "out = tf.keras.layers.LeakyReLU()(out)\n",
        "\n",
        "out = tf.keras.layers.Reshape(target_shape=(7,7,256))(out)\n",
        "\n",
        "out = tf.keras.layers.Conv2DTranspose(128,(5,5),strides=(1,1),padding=\"same\",use_bias=False)(out)\n",
        "out = tf.keras.layers.BatchNormalization()(out)\n",
        "out = tf.keras.layers.LeakyReLU()(out)\n",
        "\n",
        "out = tf.keras.layers.Conv2DTranspose(64,(5,5),strides=(2,2),padding=\"same\",use_bias=False)(out)\n",
        "out = tf.keras.layers.BatchNormalization()(out)\n",
        "out = tf.keras.layers.LeakyReLU()(out)\n",
        "\n",
        "outputs = tf.keras.layers.Conv2DTranspose(1,(5,5),strides=(2,2),use_bias=False,activation='tanh',padding=\"same\")(out)\n",
        "\n",
        "G_model = 0\n",
        "G_model=tf.keras.Model(inputs=inputs,outputs=outputs)\n",
        "G_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "642GiKVVJgCi"
      },
      "source": [
        "try_gen = G_model( tf.random.normal([1,100]) , training=False )\n",
        "print(try_gen.shape)\n",
        "show = plt.imshow( try_gen[0,:,:,0]*127.5+127.5 ,cmap='gray' )\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtokJqhSXgLI"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "raw_data,info = tfds.load('mnist',split=['train','test'],with_info=True)\n",
        "\n",
        "print(info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aC5eJBWQXixD"
      },
      "source": [
        "train_data,test_data = raw_data[0],raw_data[1]\n",
        "\n",
        "def map_func(data):\n",
        "\n",
        "  img = ( tf.cast( data['image'] , tf.float32 )-127.5 )/127.5\n",
        "  return img\n",
        "\n",
        "train_data = train_data.map( map_func )\n",
        "\n",
        "train_data = train_data.shuffle(10000)\n",
        "train_data = train_data.batch(256)\n",
        "\n",
        "for one_b in train_data.take(1):\n",
        "  show = plt.imshow(one_b[0,:,:,0],cmap='gray')\n",
        "  plt.show()\n",
        "  print( one_b.shape )\n",
        "  print( one_b[0,:,:,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWJA0nakXJEi"
      },
      "source": [
        "seed = tf.random.normal([ 16 , 100 ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGBnbwI-HDSR"
      },
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5p4JH_h2QZH"
      },
      "source": [
        "#********************\n",
        "batch_size = 256\n",
        "l_rate = 0.001\n",
        "ctr=0\n",
        "epochs=20\n",
        "# k -factor\n",
        "k=1\n",
        "#********************\n",
        "\n",
        "d_optimizer = tf.keras.optimizers.Adam( l_rate )\n",
        "g_optimizer = tf.keras.optimizers.Adam( l_rate )\n",
        "\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "# tf abracted implementation. ( Its a bit Smooth !)\n",
        "# def discriminator_loss(real_output, fake_output):\n",
        "#     real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "#     fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "#     total_loss = real_loss + fake_loss\n",
        "#     return total_loss\n",
        "\n",
        "# def generator_loss(fake_output):\n",
        "#     return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "# My implementation\n",
        "def cost_gen(pred_prob):\n",
        "\n",
        "    delta = 1e-8\n",
        "    loss = (-1.0/pred_prob.shape[0])*tf.reduce_sum( tf.math.log(  1-tf.nn.sigmoid(pred_prob) ) + delta )\n",
        "    return loss+delta\n",
        "\n",
        "def cost_data(pred_prob):\n",
        "\n",
        "    delta = 1e-8\n",
        "    loss = (-1.0/pred_prob.shape[0])*tf.reduce_sum( tf.math.log( tf.nn.sigmoid(pred_prob) ) +delta )\n",
        "    return loss+delta\n",
        "\n",
        "def cost_fn_g(pred_prob):\n",
        "    delta = 1e-8\n",
        "    loss = (-1.0/pred_prob.shape[0])*tf.reduce_sum( tf.math.log( tf.nn.sigmoid(pred_prob) ) + delta )\n",
        "    return loss+delta\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "\n",
        "  for e in range(epochs):\n",
        "    \n",
        "    print(\"********* EPOCH :: {}\".format(e))\n",
        "\n",
        "    # display.clear_output(wait=True)\n",
        "    generate_and_save_images(G_model,\n",
        "                             e + 1,\n",
        "                             seed)\n",
        "\n",
        "    g_cost = 0\n",
        "\n",
        "    for batch_true in train_data:\n",
        "      \n",
        "      batch_noise = tf.random.normal( (batch_size,100 ) )\n",
        "      \n",
        "      with tf.GradientTape(persistent=True) as tape:\n",
        "\n",
        "        pred_true = tf.squeeze( D_model( batch_true , training=True ) )\n",
        "        batch_false = G_model(batch_noise, training=True)\n",
        "        pred_false = tf.squeeze( D_model(batch_false, training=True) )\n",
        "\n",
        "        # For tf abstracted implementation\n",
        "        # d_cost = discriminator_loss(pred_true,pred_false)\n",
        "        # g_cost = generator_loss(pred_false)\n",
        "\n",
        "        cost_true = cost_data(pred_true)\n",
        "        cost_false = cost_gen(pred_false)\n",
        "\n",
        "        d_cost = (cost_true + cost_false)\n",
        "        g_cost = cost_fn_g(pred_false)\n",
        "\n",
        "\n",
        "      if ctr%100==0:\n",
        "        print(\"D_Loss is === >  {}\".format(d_cost))\n",
        "        print(\"G_Loss is === >  {}\".format(g_cost))\n",
        "\n",
        "      ctr = ctr+1\n",
        "      \n",
        "      if ctr%k==0:\n",
        "        d_grads = tape.gradient( d_cost,D_model.trainable_variables )\n",
        "        d_optimizer.apply_gradients(zip(d_grads,D_model.trainable_variables))\n",
        "\n",
        "      g_grads = tape.gradient( g_cost,G_model.trainable_variables )\n",
        "      g_optimizer.apply_gradients(zip(g_grads,G_model.trainable_variables))\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}